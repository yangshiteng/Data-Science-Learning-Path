# Introduction to Attention

# Attention in CNNs
  - [Squeeze-and-Excitation (SE)](https://github.com/yangshiteng/Data-Science-Learning-Path/blob/main/deep_learning/transformers_attention_mechanisms/se_block.md)
  - [CBAM (Channel & Spatial)]()
  - [Non-local Attention]()
  
# Attention in Sequences
  - [Seq2Seq with Attention]()
  - [Bahdanau vs Luong Attention]()
    
# Transformers
  - [Self-Attention]()
  - [Multi-Head Attention]()
  - [Encoderâ€“Decoder Framework]()
  - [BERT vs GPT]()
  - [Vision Transformers (ViT)]()
    
# Applications and Summary

### **Part 1: Introduction & Motivation**

* [1.1 What are Transformers? (Deep Learning Model Architecture with Self-attention)](https://github.com/yangshiteng/Data-Science-Learning-Path/blob/main/deep_learning/transformer/what_is_transformer.md)
* [1.2 Why were they invented? (Limitations of RNNs and CNNs for sequences)](https://github.com/yangshiteng/Data-Science-Learning-Path/blob/main/deep_learning/transformer/why_invented.md)
* [1.3 Key advantages of Transformers](https://github.com/yangshiteng/Data-Science-Learning-Path/blob/main/deep_learning/transformer/transformer_advantage.md)

---

### **Part 2: The Concept of Attention**

* 2.1 [What is Attention? (Basic idea and analogy)](https://github.com/yangshiteng/Data-Science-Learning-Path/blob/main/deep_learning/transformer/what_is_attention.md)
* 2.2 [Soft Attention vs Hard Attention](https://github.com/yangshiteng/Data-Science-Learning-Path/blob/main/deep_learning/transformer/soft_vs_hard_attention.md)
* 2.3 [Why Attention improves learning](https://github.com/yangshiteng/Data-Science-Learning-Path/blob/main/deep_learning/transformer/why_attention_improve_learning.md)

---

### **Part 3: Scaled Dot-Product Attention**

* 3.1 [Queries, Keys, and Values: The core idea](https://github.com/yangshiteng/Data-Science-Learning-Path/blob/main/deep_learning/transformer/query_key_value.md)
* 3.2 [How attention scores are computed](https://github.com/yangshiteng/Data-Science-Learning-Path/blob/main/deep_learning/transformer/attention_score_calculation.md)
* 3.3 [The scaling factor and why it matters](https://github.com/yangshiteng/Data-Science-Learning-Path/blob/main/deep_learning/transformer/why_scale_matters.md)

---

### **Part 4: Multi-Head Attention**

* 4.1 [Why use multiple attention heads?](https://github.com/yangshiteng/Data-Science-Learning-Path/blob/main/deep_learning/transformer/why_multi_head.md)
* 4.2 [What does each head learn?](https://github.com/yangshiteng/Data-Science-Learning-Path/blob/main/deep_learning/transformer/what_each_head_learn.md)
* 4.3 [How multiple heads are combined?](https://github.com/yangshiteng/Data-Science-Learning-Path/blob/main/deep_learning/transformer/multiple_head_combine.md)
* 4.4 [Multi-Head Attention Calculation](https://github.com/yangshiteng/Data-Science-Learning-Path/blob/main/deep_learning/transformer/multi_head_calculation.md)

---

### **Part 5: The Transformer Architecture**

* 5.1 [Overview of encoder and decoder](https://github.com/yangshiteng/Data-Science-Learning-Path/blob/main/deep_learning/transformer/overview_encoder_decoder.md)
* 5.2 [Key components: Layer Norm, Residuals, FFN](https://github.com/yangshiteng/Data-Science-Learning-Path/blob/main/deep_learning/transformer/key_components.md)
* 5.3 [Data flow through the model](https://github.com/yangshiteng/Data-Science-Learning-Path/blob/main/deep_learning/transformer/transformer_data_flow.md)

---

### **Part 6: Positional Encoding**

* 6.1 [Why Transformers need positional info](https://github.com/yangshiteng/Data-Science-Learning-Path/blob/main/deep_learning/transformer/why_transformer_need_position.md)
* 6.2 [Sinusoidal vs Learnable encodings]()
* 6.3 [Visualizing positional embeddings]()

---

### **Part 7: Training Transformers**

* 7.1 Input-output structure for training
* 7.2 Masking: Preventing future information
* 7.3 Loss functions and optimization

---

### **Part 8: Transformer Variants and Use Cases**

* 8.1 Encoder-only: BERT
* 8.2 Decoder-only: GPT
* 8.3 Encoder-decoder: T5, BART
* 8.4 Applications beyond text (Vision, Audio, Multimodal)

---

### **Part 9: Common Tools & Libraries**

* 9.1 Hugging Face Transformers
* 9.2 PyTorch vs TensorFlow examples
* 9.3 Visualization tools for attention

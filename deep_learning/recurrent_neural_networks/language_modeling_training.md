## ğŸŒ **Example: Training an RNN Language Model on Song Lyrics**

Letâ€™s say you want to train a model on a dataset of song lyrics to **generate new lyrics**.

---

### ğŸ“¦ **Step 1: Choose Your Dataset**

We need a dataset.
For example, letâ€™s use **Taylor Swift lyrics** collected from online sources.

We combine all her song lyrics into one big text file.

---

Example snippet:

```
Cause baby now we got bad blood
You know it used to be mad love
```

---

### ğŸ›  **Step 2: Preprocess the Data**

---

âœ… **Tokenize the text**

Decide: do we want to model **character-level** or **word-level**?

* **Character-level**: Sequence â†’ \['C', 'a', 'u', 's', 'e', ' ', 'b', 'a', 'b', 'y']
* **Word-level**: Sequence â†’ \['Cause', 'baby', 'now', 'we', 'got', 'bad', 'blood']

For this example, letâ€™s do **word-level**.

---

âœ… **Build vocabulary**

Extract all unique words and assign each one an index.

Example:

| Word    | Index |
| ------- | ----- |
| â€˜Causeâ€™ | 0     |
| â€˜babyâ€™  | 1     |
| â€˜nowâ€™   | 2     |
| â€˜weâ€™    | 3     |
| â€˜gotâ€™   | 4     |
| â€¦       | â€¦     |

Letâ€™s say we have 10,000 unique words (vocabulary size).

---

âœ… **Create inputâ€“target pairs**

We slide a **window** over the text.

For a window size of 5, example:

* Input: \[â€˜Causeâ€™, â€˜babyâ€™, â€˜nowâ€™, â€˜weâ€™, â€˜gotâ€™] â†’ Target: â€˜badâ€™
* Input: \[â€˜babyâ€™, â€˜nowâ€™, â€˜weâ€™, â€˜gotâ€™, â€˜badâ€™] â†’ Target: â€˜bloodâ€™

This forms our training dataset.

---

### ğŸ§  **Step 3: Build the RNN Model**

---

âœ… **Input layer**

* Convert word indices to dense vectors using an **embedding layer**.

For example, 10,000 vocab â†’ 128-dimensional embeddings.

---

âœ… **RNN layer**

* Use **LSTM** or **GRU** to process the input sequence.

Example:

```python
lstm = LSTM(256, return_sequences=False)
```

This outputs a hidden state summarizing the input sequence.

---

âœ… **Output layer**

* Apply a **dense (fully connected) layer** + **softmax** to predict the next word.

Example:

```python
dense = Dense(vocab_size, activation='softmax')
```

---

âœ… **Model summary**

```
Input (5 words) â†’ Embedding â†’ LSTM â†’ Dense â†’ Next word probability
```

---

### ğŸ‹ï¸ **Step 4: Define Loss Function and Optimizer**

---

âœ… **Loss function**:
Use **categorical cross-entropy** (comparing predicted vs. actual next word).

âœ… **Optimizer**:
Use **Adam** or **SGD** for efficient weight updates.

---

### ğŸ” **Step 5: Train the Model**

We loop over the dataset for multiple **epochs**.

At each batch:

1. Forward pass:

   * Input â†’ prediction (probabilities over next word).
2. Compute loss:

   * Compare prediction vs. true next word.
3. Backward pass:

   * Calculate gradients (using backpropagation through time, BPTT).
4. Update weights:

   * Apply optimizer to adjust model weights.

---

âœ… **Epoch example**

| Epoch | Average Loss |
| ----- | ------------ |
| 1     | 3.2          |
| 5     | 2.1          |
| 10    | 1.4          |

As epochs go up, loss typically goes down â€” the model gets better at predicting.

---

### ğŸ“Š **Step 6: Evaluate the Model**

---

âœ… **Quantitative metrics**

* **Perplexity** â†’ Measures how well the model predicts unseen text (lower is better).

âœ… **Qualitative check**

* Generate some **sample lyrics** and see if they look coherent.

---

### ğŸ”® **Step 7: Generate New Lyrics**

---

âœ… **Sampling process (generation)**

1. Provide a starting seed: e.g., â€œbaby now weâ€
2. Predict next word: model suggests â€˜gotâ€™
3. Append: â€œbaby now we gotâ€
4. Feed back in: â€œnow we gotâ€ â†’ predict â†’ â€˜badâ€™
5. Append: â€œbaby now we got badâ€
6. Continue for N words to generate a whole line or verse.

---

âœ… **Sampling methods**

* **Greedy search**: Always pick the top prediction.
* **Top-k sampling**: Randomly sample from the top k predictions.
* **Temperature sampling**: Adjust randomness in predictions (higher temperature = more creative).

---

### âš ï¸ **Step 8: Handle Challenges**

| Challenge                    | Solution                              |
| ---------------------------- | ------------------------------------- |
| Overfitting on training data | Use dropout or early stopping         |
| Vanishing gradients          | Use LSTM or GRU instead of simple RNN |
| Large vocabulary size        | Use techniques like sampled softmax   |
| Long-range dependencies      | Consider adding attention mechanisms  |

---

### ğŸŒŸ **What is Sampling in Language Models?**

When your model generates text, it doesnâ€™t directly **write out words** â€” it produces a **probability distribution** over the entire vocabulary for the next word.

For example, after the phrase:

> â€œthe cat sat onâ€

the model might predict:

| Word     | Probability |
| -------- | ----------- |
| â€œtheâ€    | 0.05        |
| â€œmatâ€    | 0.60        |
| â€œrugâ€    | 0.25        |
| â€œchairâ€  | 0.05        |
| â€œbananaâ€ | 0.01        |

We need a **sampling method** to **choose** which word to pick next from this distribution.

---

## ğŸ”‘ **Common Sampling Methods**

---

### 1ï¸âƒ£ **Greedy Sampling**

* Always pick the **word with the highest probability**.

From above, we would choose:

> â€œmatâ€ (60%)

âœ… **Pros**:

* Simple and fast.
* Often produces fluent, high-confidence sentences.

âš  **Cons**:

* Can get repetitive or stuck in loops.
* Misses out on creative or diverse options.

---

### 2ï¸âƒ£ **Random Sampling**

* **Randomly sample** the next word **proportional to its probability**.

Example:

* 60% â†’ â€œmatâ€ â†’ high chance.
* 25% â†’ â€œrugâ€ â†’ moderate chance.
* 1% â†’ â€œbananaâ€ â†’ very small chance (but not zero!).

âœ… **Pros**:

* Introduces variability and creativity.
* Can generate surprising or novel outputs.

âš  **Cons**:

* Might pick low-probability (nonsensical) words.
* Risk of lower overall coherence.

---

### 3ï¸âƒ£ **Top-k Sampling**

* First, **narrow down to the top k most probable words**.
* Then randomly sample **only from this shortlist**.

Example with k=2:

| Top words | Probability |
| --------- | ----------- |
| â€œmatâ€     | 0.60        |
| â€œrugâ€     | 0.25        |

Randomly sample between â€œmatâ€ and â€œrugâ€ (ignore others).

âœ… **Pros**:

* Balances between greedy and random sampling.
* Prevents strange, low-probability words from sneaking in.

âš  **Cons**:

* Needs you to tune **k** carefully.
* Still might become repetitive if k is too small.

---

### 4ï¸âƒ£ **Temperature Sampling**

* **Adjust the sharpness or softness** of the probability distribution.

Formula:

$$
P_i^{\text{new}} = \frac{P_i^{1/T}}{\sum_j P_j^{1/T}}
$$

Where:

* $T$ = temperature.

  * **T < 1** â†’ sharpen probabilities (more confident, greedy-like).
  * **T > 1** â†’ flatten probabilities (more exploratory, random).

Example:

* At **T = 0.7**, the model focuses on top words.
* At **T = 1.5**, even rare words get boosted.

âœ… **Pros**:

* Flexible control over randomness and creativity.
* Can make outputs more diverse without going totally random.

âš  **Cons**:

* Needs experimentation to pick the right **T**.

---

### ğŸ›  **How Do We Apply These?**

In code, after:

```python
predicted_probs = model.predict(token_list)
```

You can:
* âœ… Use `np.argmax(predicted_probs)` â†’ greedy sampling.
* âœ… Use `np.random.choice(vocab, p=predicted_probs)` â†’ random sampling.
* âœ… Limit to top-k â†’ manually zero out others, then rescale and sample.
* âœ… Apply temperature â†’ modify `predicted_probs` with the formula above before sampling.

---

### âœ… **Summary Table**

| Sampling Method | How It Works                                   | When To Use                   |
| --------------- | ---------------------------------------------- | ----------------------------- |
| Greedy          | Always pick most probable word                 | Coherence, correctness        |
| Random          | Sample proportional to predicted probabilities | Creativity, surprise          |
| Top-k           | Sample from top k candidates                   | Balanced creativity + safety  |
| Temperature     | Scale probability sharpness or softness        | Fine-tuned randomness control |

---

## âœ… **Summary of Full Training Process**

| Step          | Details                                      |
| ------------- | -------------------------------------------- |
| Dataset       | Collect and clean large text corpus          |
| Preprocessing | Tokenize, build vocabulary, create sequences |
| Model         | Embedding + LSTM/GRU + Dense (softmax)       |
| Training loop | Forward pass â†’ loss â†’ backprop â†’ update      |
| Evaluation    | Check loss, perplexity, generate text        |
| Sampling      | Use trained model to generate new sequences  |

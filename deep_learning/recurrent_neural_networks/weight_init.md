## âš™ï¸ **Weight Initialization Strategies**

### ğŸ§  **Why Is Weight Initialization Important?**

When training a neural network, how you **initialize the weights** has a huge impact on:

* ğŸ”„ Convergence speed
* ğŸ“‰ Whether gradients vanish or explode
* âœ… Overall model performance

Poor initialization can lead to:

* Very slow learning
* Getting stuck in bad local minima
* Instability during training

---

## ğŸ¯ **Goals of Good Initialization**

* Prevent **activations from shrinking or blowing up** layer by layer
* Keep **gradient flow stable** during backpropagation
* Maintain **symmetry breaking** (so different neurons learn different things)

---

## ğŸ”§ **Common Weight Initialization Methods**

### 1. âœ³ï¸ **Zero Initialization (Donâ€™t Do This!)**

* All weights are set to 0
* âŒ Problem: All neurons behave the same â€” no learning happens
* âœ… OK only for **biases**, not weights

---

### 2. ğŸ”€ **Random Initialization**

* Small random numbers (e.g., Gaussian or uniform)
* Better than zeros, but still not ideal â€” doesnâ€™t account for layer size
* Used mostly in early networks

---

### 3. ğŸŸ© **Xavier Initialization (Glorot Initialization)**

* Designed for **tanh** or **sigmoid** activations
* Keeps variance of activations and gradients stable across layers

For uniform distribution:

$$
W \sim U\left(-\sqrt{\frac{6}{n_{\text{in}} + n_{\text{out}}}}, \sqrt{\frac{6}{n_{\text{in}} + n_{\text{out}}}}\right)
$$

âœ… Used in: **shallow networks**, **autoencoders**, **basic RNNs**

---

### 4. ğŸ”· **He Initialization (Kaiming Initialization)**

* Designed for **ReLU** and **Leaky ReLU** activations
* Helps prevent activations from dying (outputting zero)

$$
W \sim \mathcal{N}\left(0, \frac{2}{n_{\text{in}}}\right)
$$

âœ… Used in: **CNNs**, **deep feedforward**, **ResNets**

---

### 5. ğŸ” **Orthogonal Initialization**

* Used often in **RNNs**, especially for the recurrent weight matrix $W_{hh}$
* Helps preserve **gradient magnitude over time steps**
* Makes backpropagation through time more stable

âœ… Particularly effective for long-sequence RNNs and LSTMs

---

### 6. ğŸ”„ **Identity Initialization (for RNNs)**

* Initialize $W_{hh}$ as an identity matrix
* Encourages information to **pass unchanged** across time steps early in training
* Works well with **ReLU-based RNNs**

---

## ğŸ¤– **Weight Initialization for RNNs (Recap)**

| Component                  | Recommended Strategy                            |
| -------------------------- | ----------------------------------------------- |
| Input weights $W_{xh}$     | Xavier or He Initialization                     |
| Recurrent weights $W_{hh}$ | Orthogonal or Identity Initialization           |
| Biases                     | Zeros (or small positive for LSTM forget gates) |

---

## ğŸ§¾ Summary

| Strategy         | Best For                 | Helps With                       |
| ---------------- | ------------------------ | -------------------------------- |
| Xavier           | tanh/sigmoid activations | Balanced gradients/activations   |
| He               | ReLU activations         | Avoiding dead neurons            |
| Orthogonal       | RNNs, long sequences     | Stable time-step gradient flow   |
| Identity         | ReLU-RNNs                | Preserving information over time |
| Zero (bias only) | Bias terms               | Safe for initializing biases     |

## ğŸ§® **Backpropagation Through Time (BPTT)**

### ğŸ” **1. What Is BPTT?**

**Backpropagation Through Time (BPTT)** is the extension of the standard **backpropagation** algorithm, tailored specifically for training **Recurrent Neural Networks (RNNs)**.
Because RNNs unfold over time, BPTT must also **backpropagate errors through multiple time steps**, not just layersâ€”hence the name.

ğŸ“Œ **Goal**: Adjust weights by computing gradients of the loss with respect to each parameter, considering the network's entire temporal structure.

---

### ğŸ§  **2. Why Itâ€™s Different from Standard Backprop**

In a regular neural network:

* You move forward through layers ğŸ§±
* Then backpropagate errors layer by layer

In an RNN:

* You move forward through **time steps** â©
* Then backpropagate through time ğŸ”â€”because each hidden state depends on previous ones

---

### ğŸ“ **3. How BPTT Works**

Letâ€™s say you have a sequence $x_1, x_2, ..., x_T$ with corresponding outputs $y_1, y_2, ..., y_T$ and total loss $L$ defined as:

$$
L = \sum_{t=1}^T \ell(y_t, \hat{y}_t)
$$

Then BPTT involves:

1. **Forward Pass** (as covered earlier):

   * Compute all hidden states $h_1, ..., h_T$
   * Compute predictions $\hat{y}_t$ and losses

2. **Backward Pass**:

   * Start at the final time step $T$
   * Compute gradients $\frac{\partial L}{\partial W}$, $\frac{\partial L}{\partial h_t}$, etc.
   * Accumulate gradients **through time** back to $t=1$

âœï¸ **Each weight** (e.g., $W_{hh}$) receives **a sum of gradients** from each time step where it was used.

---

### âš ï¸ **4. Challenges in BPTT**

While BPTT is powerful, it comes with issues:

* ğŸ“‰ **Vanishing Gradients**: Gradients become very small over long sequences â†’ makes learning long-term dependencies hard.
* ğŸ“ˆ **Exploding Gradients**: Gradients grow exponentially â†’ can destabilize training.

ğŸ›¡ï¸ **Solutions**:

* Gradient clipping âœ‚ï¸
* Using better architectures: **LSTM** and **GRU** are designed to mitigate these issues

---

### ğŸ§® **5. Truncated BPTT**

For long sequences, computing gradients over all time steps is computationally expensive.

â³ **Truncated BPTT** addresses this by:

* Dividing the sequence into shorter chunks (e.g., 5â€“20 steps)
* Backpropagating only within each chunk

This reduces computation while still capturing local dependencies.

---

### ğŸ” **Summary**

| Step            | Action                                     |
| --------------- | ------------------------------------------ |
| ğŸ”„ Unroll       | Duplicate RNN cell across time             |
| ğŸš€ Forward      | Compute hidden states & outputs            |
| ğŸ“‰ Compute Loss | Measure prediction error                   |
| ğŸ”™ Backward     | Backpropagate gradients through time steps |
| ğŸ› ï¸ Update      | Adjust weights using optimizer             |

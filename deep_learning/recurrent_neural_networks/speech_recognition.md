## üó£Ô∏è **What Is Speech Recognition?**

**Speech recognition** (or **Automatic Speech Recognition ‚Äì ASR**) is the task of converting **spoken language** (audio) into **written text**.

### üîç Example:

* **Input (Audio)**: Someone says *"What's the weather today?"*
* **Output (Text)**: `"what's the weather today?"`

This process must handle:

* Accents
* Background noise
* Variable speaking speeds
* Contextual word prediction

---

## üß† **Why Use RNNs for Speech Recognition?**

Speech is inherently **sequential** and **time-dependent** ‚Äî sounds unfold over time, and each sound (phoneme) depends on what came before and after.

**Recurrent Neural Networks (RNNs)** ‚Äî especially **LSTMs** and **GRUs** ‚Äî are designed to model such **temporal sequences**, making them ideal for speech recognition.

---

## üîÅ **How RNNs Work in Speech Recognition**

### üß± Basic Pipeline:

1. **Audio Input (Raw Sound)**

   * Audio waveform is recorded (a time-series signal).
   * Preprocessed into **spectrograms** or **MFCCs** (Mel-Frequency Cepstral Coefficients), which are 2D time‚Äìfrequency representations.

2. **RNN Model**

   * The spectrogram is fed into an RNN (e.g., BiLSTM).
   * The RNN processes audio **frame by frame** (like words in a sentence).
   * The RNN learns **temporal dependencies** ‚Äî which sounds lead to which letters or words.

3. **Output**

   * Model outputs either:

     * A sequence of **characters** (character-level decoding), or
     * A sequence of **words** (word-level decoding)
   * Decoding often involves **CTC (Connectionist Temporal Classification)** or attention-based decoders.

---

### üî† Character-Level RNN Example

Say we input the audio of: *"hello"*

The RNN might produce a character-by-character prediction:

| Time Frame | Predicted Output |
| ---------- | ---------------- |
| Frame 1    | h                |
| Frame 2    | e                |
| Frame 3    | l                |
| Frame 4    | l                |
| Frame 5    | o                |

---

## üî£ **Connectionist Temporal Classification (CTC)**

**CTC** is often used with RNNs for ASR when **alignment between audio and labels is unknown**.

### Why?

* Audio and text sequences are **different lengths**.
* CTC allows the model to **align predictions** over time by introducing a "blank" token and collapsing repeated characters.

### Example:

Predicted sequence:
`"h", "e", "l", "l", "o", "blank"`
‚Üí Collapsed to `"hello"`

CTC enables **end-to-end training** without needing to mark exact timing of each word or letter in the audio.

---

## üîÅ **Bidirectional RNNs in Speech Recognition**

* In **Bidirectional RNNs**, the model processes the input **forward and backward** in time.
* This helps capture **context from both past and future sounds**, improving recognition accuracy.

---

## üîÅ **Training Process Overview**

### üß† **Goal of Training**

Teach a model to **convert speech (audio)** into **written text** by learning from pairs of:

* **Audio input** (e.g., a recording of someone saying ‚Äúhello‚Äù)
* **Text output** (e.g., `"hello"`)

---

### üìö **Training Dataset**

Each training example includes:

* An **audio waveform** (speech signal)
* The **transcription** (text) of what was said

#### Example:

| Audio File      | Transcript       |
| --------------- | ---------------- |
| `audio_001.wav` | `"hello"`        |
| `audio_002.wav` | `"how are you"`  |
| `audio_003.wav` | `"i love pizza"` |
| `audio_004.wav` | `"thank you"`    |
| `audio_005.wav` | `"good morning"` |

---

### üîÅ **Step-by-Step Training Process**

---

#### **1. Preprocess the Audio**

* Convert raw audio (waveform) into a format the model can understand.
* Common methods:

  * **Spectrogram**: a visual/time-based representation of sound
  * **MFCCs (Mel-Frequency Cepstral Coefficients)**

This gives you a **2D time series** input (like an image but over time).

---

#### **2. Tokenize the Transcription**

* Break the transcription into individual **characters** (or sometimes phonemes or words).

> `"hello"` ‚Üí `['h', 'e', 'l', 'l', 'o']` ‚Üí encoded as numeric indices

* These become the output targets for training

---

#### **3. Feed Audio into an RNN (Encoder)**

* The model takes the **audio features** as a sequence.
* An RNN (like an LSTM or GRU) processes the audio **frame by frame**, building up a **hidden state** that captures what's being said over time.

---

#### **4. Decode into Text with CTC (Connectionist Temporal Classification)**

* You don‚Äôt know exactly **which part of the audio** corresponds to which character.
* So, CTC lets the model **predict multiple characters** (or blanks) at each time step and then **collapse** them into the final output.

##### Example:

The model might predict this over time:

> `['h', '-', 'e', '-', 'l', 'l', '-', 'o', '-']`
> (where `'-'` means "blank")

CTC collapses this to:

> `"hello"`

---

#### **5. Compute the Loss**

* Compare the model‚Äôs predicted output (`"hello"`) to the ground truth (`"hello"`).
* If they match, great! If not, compute the **CTC loss**, which measures how far off the prediction is.

---

#### **6. Backpropagation and Learning**

* Use **backpropagation** to update the model‚Äôs weights:

  * Adjust how it hears sounds (input encoding)
  * Improve memory of sound sequences (RNN hidden state)
  * Better predict letters over time (output decoding)

---

#### **7. Repeat for Thousands of Examples**

* This process is repeated over **many examples** and **many epochs**.
* The model gradually learns **how sound patterns map to letters/words**.

---

### üîÑ **Summary**

| Step                 | What Happens                                        |
| -------------------- | --------------------------------------------------- |
| 1. Audio Input       | Audio is converted to spectrogram or MFCC           |
| 2. Text Tokenization | Transcription is split into characters              |
| 3. Encoding          | RNN processes the audio sequence                    |
| 4. Decoding (CTC)    | Model outputs possible characters at each time step |
| 5. Loss Calculation  | Compare predicted vs. actual transcription          |
| 6. Weight Update     | Use loss to update the model via training           |

---

#### üéß Example (Training One Sample)

| Input (audio)                | Transcription |
| ---------------------------- | ------------- |
| Audio: ‚Äúhello‚Äù (spectrogram) | `"hello"`     |

Prediction:

* RNN outputs: `'h', '-', 'e', '-', 'l', 'l', '-', 'o', '-'`
* Collapsed with CTC: `"hello"`
* If prediction ‚â† ground truth, update model.

---

## üí° Advanced Architectures

Modern ASR systems often build on RNNs with improvements:

* **Deep RNNs**: Stacked LSTM/GRU layers for richer representations.
* **Attention Mechanisms**: For focusing on relevant parts of the input.
* **Encoder‚ÄìDecoder Models**: Especially useful in speech-to-text tasks.
* **Transformers** (replacing RNNs in some newer systems)

But RNNs are still foundational, especially for streaming or real-time ASR.

---

## üîç **Real-World Use Cases**

| Application                      | How RNNs Are Used                       |
| -------------------------------- | --------------------------------------- |
| **Voice Assistants**             | Siri, Alexa, Google Assistant           |
| **Live Captioning**              | Subtitles in Zoom, YouTube, Google Meet |
| **Voice Search**                 | Search by voice in browsers or apps     |
| **Call Center Transcripts**      | Convert customer calls to text          |
| **Voice Commands**               | Smart home control, in-car assistants   |
| **Speech-to-Speech Translation** | First step is speech-to-text            |

---

## üìä Evaluation Metrics

| Metric                         | Description                                                             |
| ------------------------------ | ----------------------------------------------------------------------- |
| **WER (Word Error Rate)**      | % of words incorrectly predicted (insertions, deletions, substitutions) |
| **CER (Character Error Rate)** | Especially for character-level decoding                                 |
| **Accuracy**                   | % of correctly predicted words                                          |
| **Latency**                    | Time taken to process audio and produce text (important for real-time)  |

---

## ‚úÖ Summary Table

| Feature                  | Description                                       |
| ------------------------ | ------------------------------------------------- |
| **Task**                 | Convert speech audio to text                      |
| **Input**                | Preprocessed audio (e.g., spectrograms)           |
| **Model**                | RNN / LSTM / GRU / BiLSTM (with CTC or attention) |
| **Output**               | Sequence of characters or words                   |
| **Key Strength of RNNs** | Captures time dependencies in audio               |
| **Used in**              | Voice assistants, transcription, accessibility    |

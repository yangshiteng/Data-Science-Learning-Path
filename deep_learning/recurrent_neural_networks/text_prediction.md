## ‚ú® **What is Text Prediction & Autocomplete?**

**Text prediction** and **autocomplete** are features that **suggest the next word or phrase** as a user types. They're common in:

* Smartphone keyboards (e.g., Gboard, SwiftKey)
* Email clients (e.g., Gmail‚Äôs Smart Compose)
* Coding tools (e.g., GitHub Copilot, IDEs)
* Messaging apps (e.g., WhatsApp quick replies)

---

## üß† **Why Use RNNs?**

Text prediction requires **understanding the context of a sentence** over time. RNNs are built for this:

* They process input **sequentially**, word by word or character by character.
* They maintain a **hidden state** that carries context from previous inputs.
* Variants like **LSTMs** and **GRUs** help handle longer contexts by preserving long-term dependencies.

---

## üîÅ **How It Works: Step-by-Step**

Let‚Äôs walk through how an RNN-based autocomplete system works.

---

### 1. **Input Text Sequence**

The user types a partial sentence:

> `"I would like to"`

---

### 2. **Text Preprocessing**

* The input is **tokenized** (e.g., into words or subwords).
* Each token is converted into a **vector** (embedding or one-hot encoding).
* These vectors are fed into the RNN **one at a time**.

---

### 3. **Prediction**

The RNN (often an LSTM/GRU):

* Updates its **hidden state** at each step.
* At the final time step, predicts a **probability distribution over the vocabulary** for the next word.

Example output:

| Word    | Probability |
| ------- | ----------- |
| `eat`   | 0.35        |
| `go`    | 0.30        |
| `sleep` | 0.20        |
| `drive` | 0.15        |

The system can:

* Suggest the **top word** (`"eat"`)
* Or show **top-k options** (`"eat"`, `"go"`, `"sleep"`)

---

### 4. **Output Display**

The predicted word or phrase is shown as a suggestion:

> **Suggestion:** `"eat"`
> Full prediction: `"I would like to eat"`

Users can:

* Tap to accept
* Continue typing to ignore

---

## üß† **Training the RNN Model**

Train an RNN to **predict the next word** given a sequence of previous words ‚Äî like `"I want to"` ‚Üí predict: `"eat"`.

This is the core idea behind autocomplete and predictive typing systems.

---

### üìö **Step 1: Training Dataset (Simple Example)**

Let‚Äôs use a **very small dataset** with just 3 example sentences:

```
1. I want to eat
2. I want to sleep
3. I like pizza
```

#### üîÅ Convert Sentences into Input‚ÄìTarget Pairs

For training, we split each sentence into:

* **Input sequence (context)**: previous words
* **Target word**: next word to predict

| Input Text  | Target  |
| ----------- | ------- |
| "I"         | "want"  |
| "I want"    | "to"    |
| "I want to" | "eat"   |
| "I want to" | "sleep" |
| "I like"    | "pizza" |

Note: The input length is often fixed (e.g., 2 or 3 words), and the model learns to predict the next word.

---

### üß± **Step 2: Preprocessing for the Model**

#### üîπ Tokenization

We assign each word a unique index:

| Word    | Index |
| ------- | ----- |
| "I"     | 1     |
| "want"  | 2     |
| "to"    | 3     |
| "eat"   | 4     |
| "sleep" | 5     |
| "like"  | 6     |
| "pizza" | 7     |

So, `"I want to"` ‚Üí `[1, 2, 3]`

---

### üß† **Step 3: RNN Training Process**

#### üîπ Input

The model receives sequences of word indices (or their embeddings).

#### üîπ Model Structure

* **Embedding layer**: Translates word indices to dense vectors.
* **RNN/LSTM layer**: Processes the sequence step-by-step and maintains context.
* **Dense + Softmax layer**: Predicts a probability distribution over the vocabulary for the **next word**.

#### üîπ Training Step

For each input‚Äìtarget pair:

1. The input sequence (e.g., `"I want to"`) is passed through the model.
2. The model predicts the **next word**.
3. It compares the prediction to the actual target (e.g., `"eat"`).
4. It calculates the **loss** (usually categorical cross-entropy).
5. It updates the model weights via **backpropagation**.

---

### üß™ **Example Training Instance**

* Input: `"I want to"` ‚Üí `[1, 2, 3]`
* Target: `"eat"` ‚Üí index `4`

The model outputs a prediction like this:

| Word    | Probability |
| ------- | ----------- |
| "eat"   | 0.60        |
| "sleep" | 0.30        |
| "pizza" | 0.10        |

Since the true answer is `"eat"` (index `4`), the loss is computed between this distribution and the target, and the model learns to **increase the probability for the correct word**.

---

### üîÑ **Repeat Across Dataset**

This process is repeated over **thousands to millions of sentence pairs**, allowing the model to learn language patterns and generate accurate predictions over time.

---

### ‚úÖ **Summary**

| Step              | What Happens                                           |
| ----------------- | ------------------------------------------------------ |
| **Dataset**       | Sentences split into input‚Äìtarget pairs                |
| **Preprocessing** | Tokenize and convert words to numbers                  |
| **Input**         | Sequence of words (e.g., `"I want to"`)                |
| **Target**        | Next word (e.g., `"eat"`)                              |
| **Model**         | RNN/LSTM + Softmax                                     |
| **Training**      | Predict next word, compare with target, update weights |
| **Goal**          | Minimize error in next-word prediction                 |

---

## üõ†Ô∏è **Model Architecture**

A basic RNN-based text predictor might include:

| Layer                | Function                                     |
| -------------------- | -------------------------------------------- |
| **Embedding Layer**  | Converts words to vector representations     |
| **RNN / LSTM / GRU** | Processes sequence and maintains context     |
| **Dense Layer**      | Maps RNN output to vocabulary size           |
| **Softmax Layer**    | Produces probability distribution over words |

---

## ‚öôÔ∏è **Inference vs. Training**

| Phase         | Input                   | Output                                      |
| ------------- | ----------------------- | ------------------------------------------- |
| **Training**  | Full sentence sequences | Predict next token, learn from ground truth |
| **Inference** | Partial user input      | Suggest likely next word(s)                 |

---

## üîç **Use Cases in the Real World**

| Application                     | Description                                            |
| ------------------------------- | ------------------------------------------------------ |
| **Smartphones**                 | Predict next word as you type                          |
| **Email (Gmail Smart Compose)** | Suggests full phrases (e.g., "Thanks for your email")  |
| **IDEs and Code Editors**       | Autocomplete variables, methods, or entire code blocks |
| **Search Engines**              | Predict the user‚Äôs query in the search bar             |

---

## üìä **Evaluation Metrics**

| Metric                | Description                                             |
| --------------------- | ------------------------------------------------------- |
| **Top-1 Accuracy**    | % of times the first suggestion is correct              |
| **Top-k Accuracy**    | % of times the correct word is in the top k suggestions |
| **Perplexity**        | Measures how confident the model is in its predictions  |
| **Keystroke Savings** | Measures typing effort saved by using autocomplete      |

---

## ‚úÖ **Summary Table**

| Feature                | Description                                    |
| ---------------------- | ---------------------------------------------- |
| **Task**               | Predict or autocomplete next word/phrase       |
| **Input**              | Partial text typed by the user                 |
| **Output**             | Suggested next token(s)                        |
| **Model Type**         | RNN / LSTM / GRU                               |
| **Training Objective** | Minimize prediction error on next token        |
| **Real-World Tools**   | Smart Compose, predictive keyboards, code IDEs |

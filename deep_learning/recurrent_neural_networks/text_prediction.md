## ‚ú® **What is Text Prediction & Autocomplete?**

**Text prediction** and **autocomplete** are features that **suggest the next word or phrase** as a user types. They're common in:

* Smartphone keyboards (e.g., Gboard, SwiftKey)
* Email clients (e.g., Gmail‚Äôs Smart Compose)
* Coding tools (e.g., GitHub Copilot, IDEs)
* Messaging apps (e.g., WhatsApp quick replies)

---

## üß† **Why Use RNNs?**

Text prediction requires **understanding the context of a sentence** over time. RNNs are built for this:

* They process input **sequentially**, word by word or character by character.
* They maintain a **hidden state** that carries context from previous inputs.
* Variants like **LSTMs** and **GRUs** help handle longer contexts by preserving long-term dependencies.

---

## üîÅ **How It Works: Step-by-Step**

Let‚Äôs walk through how an RNN-based autocomplete system works.

---

### 1. **Input Text Sequence**

The user types a partial sentence:

> `"I would like to"`

---

### 2. **Text Preprocessing**

* The input is **tokenized** (e.g., into words or subwords).
* Each token is converted into a **vector** (embedding or one-hot encoding).
* These vectors are fed into the RNN **one at a time**.

---

### 3. **Prediction**

The RNN (often an LSTM/GRU):

* Updates its **hidden state** at each step.
* At the final time step, predicts a **probability distribution over the vocabulary** for the next word.

Example output:

| Word    | Probability |
| ------- | ----------- |
| `eat`   | 0.35        |
| `go`    | 0.30        |
| `sleep` | 0.20        |
| `drive` | 0.15        |

The system can:

* Suggest the **top word** (`"eat"`)
* Or show **top-k options** (`"eat"`, `"go"`, `"sleep"`)

---

### 4. **Output Display**

The predicted word or phrase is shown as a suggestion:

> **Suggestion:** `"eat"`
> Full prediction: `"I would like to eat"`

Users can:

* Tap to accept
* Continue typing to ignore

---

## üß† **Training the RNN Model**

To train such a system, we use a **language modeling setup**:

### Training Data:

* Sentences from a large corpus (e.g., emails, articles, messages)

### Input‚ÄìTarget Pairs:

For sentence: `"I would like to eat pizza"`

We create:

* Input: `"I"` ‚Üí Target: `"would"`
* Input: `"I would"` ‚Üí Target: `"like"`
* ...
* Input: `"I would like to"` ‚Üí Target: `"eat"`

### Loss:

* **Cross-entropy loss** is used to compare the predicted word with the correct one.
* Optimizer (e.g., Adam) updates the model based on this loss.

---

## üõ†Ô∏è **Model Architecture**

A basic RNN-based text predictor might include:

| Layer                | Function                                     |
| -------------------- | -------------------------------------------- |
| **Embedding Layer**  | Converts words to vector representations     |
| **RNN / LSTM / GRU** | Processes sequence and maintains context     |
| **Dense Layer**      | Maps RNN output to vocabulary size           |
| **Softmax Layer**    | Produces probability distribution over words |

---

## ‚öôÔ∏è **Inference vs. Training**

| Phase         | Input                   | Output                                      |
| ------------- | ----------------------- | ------------------------------------------- |
| **Training**  | Full sentence sequences | Predict next token, learn from ground truth |
| **Inference** | Partial user input      | Suggest likely next word(s)                 |

---

## üîç **Use Cases in the Real World**

| Application                     | Description                                            |
| ------------------------------- | ------------------------------------------------------ |
| **Smartphones**                 | Predict next word as you type                          |
| **Email (Gmail Smart Compose)** | Suggests full phrases (e.g., "Thanks for your email")  |
| **IDEs and Code Editors**       | Autocomplete variables, methods, or entire code blocks |
| **Search Engines**              | Predict the user‚Äôs query in the search bar             |

---

## üìä **Evaluation Metrics**

| Metric                | Description                                             |
| --------------------- | ------------------------------------------------------- |
| **Top-1 Accuracy**    | % of times the first suggestion is correct              |
| **Top-k Accuracy**    | % of times the correct word is in the top k suggestions |
| **Perplexity**        | Measures how confident the model is in its predictions  |
| **Keystroke Savings** | Measures typing effort saved by using autocomplete      |

---

## ‚úÖ **Summary Table**

| Feature                | Description                                    |
| ---------------------- | ---------------------------------------------- |
| **Task**               | Predict or autocomplete next word/phrase       |
| **Input**              | Partial text typed by the user                 |
| **Output**             | Suggested next token(s)                        |
| **Model Type**         | RNN / LSTM / GRU                               |
| **Training Objective** | Minimize prediction error on next token        |
| **Real-World Tools**   | Smart Compose, predictive keyboards, code IDEs |

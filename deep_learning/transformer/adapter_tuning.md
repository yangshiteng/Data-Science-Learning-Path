# ğŸ”Œ **Adapter Tuning in Transformers**

---

## ğŸ§  What Is Adapter Tuning?

**Adapter tuning** is a method for **efficiently fine-tuning large pretrained Transformer models** by inserting **small trainable layers (adapters)** between the frozen layers of the model.
Instead of updating all model parameters, **only the adapters are trained** for a given task.

> ğŸ“Œ The rest of the model (e.g., BERT, T5, GPT) stays **frozen**.
> ğŸ§© Only the **adapters** learn task-specific knowledge.

---

## ğŸ§© Why Adapter Tuning?

Training huge Transformer models (like BERT with 110M or GPT-3 with 175B parameters) is:

* ğŸ›‘ **Resource-intensive**
* ğŸ›‘ **Slow**
* ğŸ›‘ **Hard to scale across tasks**

Adapter tuning solves this by:

* âœ… Reducing trainable parameters to **<5%** of the model
* âœ… Allowing **multi-task** or **domain adaptation** efficiently
* âœ… Preserving the **general knowledge** of the base model

---

## âš™ï¸ How Adapter Tuning Works â€“ Step by Step

---

### ğŸ”¹ Step 1: Start with a Pretrained Model

Use a pretrained model such as:

* BERT, RoBERTa (Encoder-based)
* T5, BART (Encoderâ€“Decoder)
* GPT (Decoder-based)

**Do not modify or unfreeze it**.

---

### ğŸ”¹ Step 2: Insert Adapter Modules into Each Layer

Adapters are small bottleneck layers added between layers of the Transformer:

```
[Input]
   â†“
LayerNorm
   â†“
â†’ [Adapter: Down â†’ Nonlinear â†’ Up] â†’ Add & Norm â†’
   â†“
[Next Layer]
```

Each adapter typically has:

* **Down-projection**: reduces dimensionality (e.g., 768 â†’ 64)
* **Activation**: e.g., ReLU or GELU
* **Up-projection**: restores dimension (64 â†’ 768)

ğŸ” This bottleneck structure:

* Adds capacity for learning
* Keeps the parameter count small

---

### ğŸ”¹ Step 3: Train Only the Adapters

* Freeze the base model parameters (weights are not updated)
* **Only train the small adapter modules**
* Optionally train a **task head** (like a classifier)

---

## ğŸ§  Adapter Module Architecture

Letâ€™s assume base hidden size $d = 768$, adapter bottleneck size $m = 64$:

1. Input: $h \in \mathbb{R}^{768}$
2. Down-projection: $W_{down} \in \mathbb{R}^{64 \times 768}$
3. Nonlinearity: ReLU or GELU
4. Up-projection: $W_{up} \in \mathbb{R}^{768 \times 64}$
5. Output: $h + W_{up}(f(W_{down}(h)))$

> ğŸ” This residual connection ensures stability and compatibility.

---

## ğŸ“Š Why This Works

Transformers already have **rich pretrained knowledge**.
Adapters provide **just enough capacity** to specialize in a new task without overwriting general knowledge.

---

## ğŸ› ï¸ Implementation Options

### ğŸ“¦ Tools & Libraries:

* **AdapterHub**: Plug-and-play adapters for Hugging Face models
* **Hugging Face PEFT** (`peft.adapters`)
* **transformers.adapters** (`model.add_adapter(...)`)

### ğŸ§ª Training Setup:

* Optimizer: AdamW
* LR: Usually higher than fine-tuning (since it's a small set)
* Training time: 10x faster than full fine-tuning
* Output: A few MB per adapter

---

## ğŸ§ª Use Cases

| Scenario                         | Benefit                            |
| -------------------------------- | ---------------------------------- |
| âœ… Fine-tuning on limited compute | Lightweight tuning only            |
| âœ… Multi-task learning            | One model, many adapters           |
| âœ… Domain-specific NLP            | Custom adapters for each domain    |
| âœ… Privacy (edge deployment)      | Only send adapter, not whole model |
| âœ… Continual learning             | Add adapters instead of retraining |

---

## ğŸ“¦ Example: Multi-task Adapter Tuning with BERT

| Task       | Adapter Module           |
| ---------- | ------------------------ |
| Sentiment  | `bert_sentiment_adapter` |
| NER        | `bert_ner_adapter`       |
| Medical QA | `bert_medqa_adapter`     |

All adapters share the **same frozen base model** â†’ modular & efficient.

---

## ğŸ“‰ Performance vs. Efficiency

| Method           | Accuracy  | Trainable Params | Speed        |
| ---------------- | --------- | ---------------- | ------------ |
| Full Fine-Tuning | â­â­â­â­â­     | 100%             | âŒ Slow       |
| Adapter Tuning   | â­â­â­â­      | \~1â€“5%           | âœ… Fast       |
| Prompt Tuning    | â­â­ to â­â­â­ | <1%              | âœ…âœ… Very Fast |

ğŸ“Œ Adapter tuning **approaches full fine-tuning** performance while using much less memory and training time.

---

## ğŸ“ Storage and Reuse

* Fine-tuned model = 400MB+
* Adapter = 5â€“20MB
* You can keep hundreds of adapters and **swap** them on-demand without reloading the base model.

---

## ğŸ§  One-Liner Summary:

> **Adapter tuning** is a modular, parameter-efficient tuning strategy that **adds small learnable layers** to a frozen Transformer model â€” letting you train fast, switch tasks, and scale affordably without touching the base model.

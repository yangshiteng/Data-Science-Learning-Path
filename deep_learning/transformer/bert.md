### ðŸ” **Part 8.1: Encoder-Only Transformers â€“ Introduction to BERT**

---

### âœ… **Quick Summary:**

**BERT** (Bidirectional Encoder Representations from Transformers) is a **Transformer-based model** that uses **only the encoder** stack.
Itâ€™s designed to deeply understand language by looking at **all words in context**, both to the left and right â€” hence â€œbidirectional.â€

---

## ðŸ§  **What is BERT?**

* Developed by Google in 2018
* One of the first models to **pre-train deep contextual representations** using Transformers
* Pre-trained on a large corpus (Wikipedia + BooksCorpus)
* Powers many NLP applications: search, question answering, chatbots, etc.

---

## ðŸ§± **BERT Architecture (Encoder-Only)**

| Component | Description                                                          |
| --------- | -------------------------------------------------------------------- |
| Input     | Tokenized sentence(s) + segment IDs                                  |
| Stack     | Multiple layers of Transformer **encoders** (e.g., 12 for BERT-base) |
| Output    | One embedding per input token                                        |

ðŸ“Œ No decoder â†’ BERT does **not generate** sequences â€” it **understands** them.

---

## ðŸ” **Training Objectives (How BERT Learns)**

### 1. **Masked Language Modeling (MLM)**

* Randomly masks 15% of input tokens during training
* Model learns to **predict the masked words**
* Example:

  > Input: *â€œThe \[MASK] sat on the mat.â€*
  > Predict: *â€œcatâ€*

This forces BERT to understand the **full context** of the sentence.

### 2. **Next Sentence Prediction (NSP)**

* Given two sentences, BERT predicts whether the second sentence **follows the first** in the original text.
* Helps BERT understand sentence-to-sentence relationships

---

## ðŸ’¼ **What BERT is Good At**

| Task                    | Example                            |
| ----------------------- | ---------------------------------- |
| Sentence classification | Spam detection, sentiment analysis |
| Token classification    | Named Entity Recognition (NER)     |
| Question answering      | SQuAD                              |
| Sentence similarity     | Semantic search                    |
| Text embedding          | Universal sentence representations |

---

## ðŸ”„ **How BERT Is Used**

BERT is typically **fine-tuned** on downstream tasks:

1. Add a small task-specific head (e.g., a classifier)
2. Train on your labeled data (e.g., IMDB sentiment dataset)
3. Use BERTâ€™s encoder layers to provide powerful features

---

### ðŸ§  One-Liner Summary:

> **BERT** is an **encoder-only Transformer** trained to understand text deeply using **masked language modeling** â€” ideal for classification, QA, and language understanding tasks.

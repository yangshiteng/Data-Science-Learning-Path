### ðŸ” **Key Components of the Transformer Block**

**Layer Normalization, Residual Connections, and Feedforward Layers**

---

In both **encoder** and **decoder** blocks, there are 3 key building blocks that ensure the model trains well and generalizes effectively:

---

## ðŸ§± 1. Residual Connections (Skip Connections)

### âœ… **What it is:**

A **shortcut** that adds the input of a layer directly to its output before passing to the next step.

### ðŸ“ **Formula:**

$$
\text{Output} = x + \text{SubLayer}(x)
$$

### ðŸ” **Why it matters:**

* Helps gradients flow during backpropagation
* Prevents "vanishing gradients"
* Encourages **identity learning** â€” the model can choose to pass information unchanged if needed

> ðŸ“˜ Think of this like saying: "Letâ€™s do something new with the input, **but keep a copy of the original just in case**."

---

## ðŸ§ª 2. Layer Normalization (LayerNorm)

### âœ… **What it is:**

A normalization technique that **stabilizes** the outputs across each token's feature vector.

### ðŸ” **Why it matters:**

* Speeds up training
* Makes the model **less sensitive** to weight initialization
* Works well with residuals

### ðŸ“ **Where it's used:**

Typically applied **after the residual connection**:

$$
\text{Output} = \text{LayerNorm}(x + \text{SubLayer}(x))
$$

> ðŸ§  Unlike batch normalization, which works across a batch, LayerNorm works **within each token vector**.

---

## ðŸ” 3. Feedforward Network (FFN or MLP)

### âœ… **What it is:**

A simple **two-layer fully connected neural network** applied **independently to each token**.

### ðŸ“ **Formula:**

$$
\text{FFN}(x) = \text{ReLU}(xW_1 + b_1)W_2 + b_2
$$

### ðŸ” **Why it matters:**

* Adds **non-linearity** and **depth** beyond attention
* Helps the model learn **richer representations** for each token

> ðŸ“˜ Even though attention captures relationships **between tokens**, the FFN helps process each token **individually** with more flexibility.

---

## ðŸ§± Summary: Transformer Block Structure

For each encoder or decoder layer:

```
1. SubLayer 1: Multi-head attention
   â†’ Add + LayerNorm

2. SubLayer 2: Feedforward network
   â†’ Add + LayerNorm
```

---

### ðŸ§  One-Liner Summary:

> **Residuals help information flow**, **LayerNorm keeps it stable**, and **Feedforward layers add learning depth** â€” all working together to make the Transformer powerful and trainable.

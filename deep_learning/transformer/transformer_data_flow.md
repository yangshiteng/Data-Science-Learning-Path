### ğŸ” **Data Flow Through the Transformer**

![image](https://github.com/user-attachments/assets/3700dac7-fd6c-4a87-bbfb-2961731d903b)

---

## ğŸ” **High-Level Overview**

### ğŸ¯ Input:

A source sequence (e.g., a sentence in English):

> *â€œThe cat sat.â€*

### ğŸ¯ Output:

A target sequence (e.g., translation in French):

> *â€œLe chat s'est assis.â€*

---

## ğŸ§­ **Step-by-Step Data Flow**

---

### ğŸ”¹ **1. Input Tokenization & Embedding**

* Text is broken into tokens (e.g., â€œTheâ€, â€œcatâ€, â€œsatâ€ â†’ \[101, 402, 578])
* Each token is converted into a **dense vector (embedding)**.
* **Positional encoding** is added to inject word order.

$$
\text{Input} = \text{Embedding}(\text{Tokens}) + \text{Positional Encoding}
$$

---

### ğŸ”¹ **2. Encoder Stack**

Each token embedding is passed through **N identical encoder layers**.

For each encoder layer:

1. **Multi-head self-attention**:

   * Each token can **attend to all others** in the input.
   * Builds a **context-aware representation** of each word.

2. **Residual connection + LayerNorm**

3. **Feedforward network (FFN)**:

   * Applied independently to each token.

4. **Another residual + LayerNorm**

âœ… **Output**: A sequence of **context-rich vectors**, one per input token.

---

### ğŸ”¹ **3. Decoder Input Prep**

* Decoder also receives tokenized input (the translation so far)
* During training, this is the target sentence shifted right
  (e.g., start with \[â€œ<s>â€, â€œLeâ€, â€œchatâ€] â†’ predict â€œs'estâ€, then â€œassisâ€, etc.)
* Decoder input is also embedded + positional encoding added

---

### ğŸ”¹ **4. Decoder Stack**

Each decoder layer has **three sub-layers**:

1. **Masked Multi-head self-attention**

   * Prevents each token from seeing the **future** (auto-regression)
   * Example: At time step 3, only tokens 1â€“3 are visible

2. **Encoderâ€“Decoder attention**

   * Decoder tokens **attend to encoder outputs**
   * Helps the model relate the translated words back to the source

3. **Feedforward network + residuals + normalization**

âœ… Output: A sequence of vectors, one per decoder token

---

### ğŸ”¹ **5. Final Linear + Softmax**

* Decoder output is passed through a **linear projection layer**
* Then through **softmax** to get a probability distribution over the vocabulary
* The **highest probability token is chosen** (during inference)

---

## ğŸ“Š **Visual Flow Summary**

```
Input Sequence (tokens)
      â†“
Embeddings + Positional Encoding
      â†“
[ Encoder Layers (Self-Attn + FFN) ]
      â†“
Contextualized Encoder Outputs
      â†“                             â†‘
Decoder Inputs (shifted targets)   |
      â†“                            |
Embeddings + Positional Encoding   |
      â†“                            |
[ Decoder Layers:                 ]
  - Masked Self-Attn              |
  - Encoderâ€“Decoder Attention â†â”€â”€â”€
  - FFN                           ]
      â†“
Final Linear â†’ Softmax â†’ Output Tokens
```

---

### ğŸ§  One-Liner Summary:

> In a Transformer, data flows through layers of **attention and feedforward networks**, with encoders building rich context and decoders generating outputs step-by-step using that context.

# ðŸŽ¯ **Parameters Optimized During CNN Training**

CNNs learn **two main types of parameters**:  
1. **Weights**
2. **Biases**

These exist across different layers of the network:

---

## **1. Convolutional Layer Parameters**

- âœ… **Filter (Kernel) Weights**  
  - Each filter contains weights that define how it detects features.
  - Shape: `(filter height Ã— filter width Ã— input channels)`
  - If you have 32 filters of size `3Ã—3Ã—3`, you have `32 Ã— (3Ã—3Ã—3) = 864` weights to learn.

- âœ… **Bias Terms**  
  - One bias per filter.
  - Allows flexibility in feature detection even with zero input.
  - For 32 filters â†’ 32 bias terms.

- âœ… **Convolutional Layers Do not Share Parameters**
  - Each convolutional layer in a CNN has its own unique set of filters (weights and biases). The filters in one layer are completely independent of the filters in other layers.
  - For example, if you have three Convolutional Layers,
    - Layer 1 filters learn low-level features (edges, gradients, corners)
    - Layer 2 filters combine these into mid-level features (textures, shapes)
    - Layer 3 filters detect high-level features (eyes, wheels, faces)
      
- âœ… **What is Shared Then?**
  - Within the same layer, a filterâ€™s weights are shared across spatial locations.
  - This is called parameter sharing: A filter slides across the input and uses the same weights at every position in that layer.

---

## **2. Fully Connected Layer Parameters**

- âœ… **Weights (Matrix)**  
  - Connect every neuron in one layer to every neuron in the next.
  - Shape: `(number of neurons in previous layer Ã— number of neurons in current layer)`

- âœ… **Biases (Vector)**  
  - One bias for each neuron in the layer.
  - Allows shifting the activation functionâ€™s threshold.

---

## **3. Parameters in Optional Components**

- âœ… **Batch Normalization** (if used)
  - Learnable parameters:  
    - **Î³ (scale)** and **Î² (shift)** for each feature channel.
    - Helps the network adaptively normalize intermediate outputs.

- âœ… **Parametric Activation Functions** (like PReLU)
  - Learnable slope values for negative activations.
  - Adds flexibility to activation behavior.

---

# âœ… **Summary Table**

| **Layer Type**            | **Optimized Parameters**                            |
|---------------------------|-----------------------------------------------------|
| **Convolutional Layer**   | Filter Weights, Filter Biases                      |
| **Fully Connected Layer** | Weight Matrix, Bias Vector                         |
| **Batch Normalization**   | Scale (Î³), Shift (Î²)                                |
| **PReLU / Learnable ReLU**| Negative slope values                              |

---

# ðŸ“Œ Whatâ€™s *Not* Learned (Fixed)

- **Stride**
- **Padding**
- **Filter size**
- **Pooling operations** (max/avg) â€” these are fixed algorithms, not learnable

These are **manually set** and control how the network processes data:

## 1. **Filter Size** (e.g., 3Ã—3, 5Ã—5)
- Common default: **3Ã—3**
- Larger filters (e.g., 5Ã—5 or 7Ã—7) can capture more context but increase computation.
- Smaller filters stacked in multiple layers can achieve similar results with fewer parameters.

> âœ… Usually chosen based on architecture design or experimentation.

---

## 2. **Stride** (e.g., 1 or 2)
- Stride = 1 â†’ preserves more spatial detail.
- Stride = 2 â†’ downsamples the feature map, reducing size and computation.
- Used in convolution and pooling layers.

> âœ… Tuned to balance **speed vs. accuracy**.

---

## 3. **Padding** (`valid` or `same`)
- **"Same" padding**: keeps output size equal to input (if stride = 1).
- **"Valid" padding**: no padding, output shrinks.
  
> âœ… Choice depends on whether preserving spatial size is important.

---

## 4. **Pooling Type and Size**
- **Max pooling** (most common): picks the max value in a region.
- **Average pooling**: takes the average.
- Pool size (e.g., 2Ã—2), stride usually = pool size.

> âœ… Typically fixed across layers (e.g., 2Ã—2 max pooling), but can be tuned.

---

## ðŸŽ¯ How Do We Choose These?

### âœ… **Hyperparameter Tuning**
You can choose values through:
- **Trial-and-error** (common in early experimentation)
- **Grid search / Random search**
- **Bayesian optimization / AutoML tools**
- **Validation performance** (using a held-out dataset)

---

### âœ… Example Tuning Setup

| **Hyperparameter** | **Common Choices**        |
|--------------------|---------------------------|
| Filter Size        | 3Ã—3, 5Ã—5                  |
| Stride             | 1, 2                      |
| Padding            | "same", "valid"           |
| Pooling Type       | Max, Average              |
| Pooling Size       | 2Ã—2, 3Ã—3                  |
| Activation Func    | ReLU, Leaky ReLU          |

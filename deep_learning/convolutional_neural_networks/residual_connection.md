# ðŸ“š **Residual Connections (ResNet Blocks)**

---

# ðŸ§  **What Are Residual Connections?**

> A **residual connection** (or **skip connection**) is a shortcut that **bypasses one or more layers** in a neural network and **adds the input directly to the output** of those layers.

âœ… This allows the network to **learn only the difference (residual)** between input and output â€” not the full transformation.

---

## ðŸ”Ž **Why Were They Introduced?**

* As networks got deeper (50+ layers), **training became difficult** due to:

  * **Vanishing gradients**
  * **Degradation** (more layers = worse accuracy)
* Deep networks struggled to **propagate gradients** back through many layers.

> âž¤ He et al. (2015) introduced **ResNet (Residual Networks)** to allow training of **very deep CNNs** (up to 152 layers) without degradation.

---

# ðŸ”§ **How Residual Connection Works**

Suppose you have some input `x` going through a few layers (e.g., Conv â†’ ReLU â†’ Conv), and it produces `F(x)`.

### Instead of just outputting `F(x)`, a residual block outputs:

```python
Output = F(x) + x
```

âœ… The **input is added ("skipped") to the output** of the layer(s).

âœ… Then typically passed through an activation function (like ReLU).

---

## ðŸ” **Visual Representation of a Residual Block**

```
Input (x)
   â”‚
[ Conv â†’ BN â†’ ReLU â†’ Conv â†’ BN ] = F(x)
   â”‚
Skip connection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                             â–¼
                 Add: F(x) + x
                         â”‚
                      ReLU
                         â–¼
                     Output
```

âœ… This is the classic **ResNet Basic Block**.

---

# ðŸ§ª **Why Does This Help?**

### 1. **Easier to Learn Identity Function**

If extra layers arenâ€™t needed, the model can **just learn to output zeros**, so:

```
F(x) = 0 â†’ Output = x
```

âœ… This avoids hurting performance when adding more layers.

---

### 2. **Better Gradient Flow**

* The **skip connection lets gradients flow back directly** to earlier layers.
* This solves the **vanishing gradient problem** in very deep networks.

---

# ðŸ›ï¸ **Used In:**

| Model                         | Uses Residual Connections?              |
| ----------------------------- | --------------------------------------- |
| **ResNet (18/34/50/101/152)** | âœ… Yes (core architecture)               |
| **DenseNet**                  | âœ… Similar idea (dense skip connections) |
| **EfficientNet**              | âœ… Uses residual blocks                  |
| **MobileNetV2**               | âœ… Inverted residual blocks              |

---

# ðŸ§  **Mathematical View**

Let:

* Input = `x`
* Learned function = `F(x)`
* Output = `y`

Then:

```
y = F(x) + x
```

Instead of learning `y = H(x)`,
The network learns `F(x) = H(x) â€“ x`, which is the **residual**.

âœ… Often easier to learn a **change (delta)** than to learn the **whole mapping**.

---

# ðŸ“Š **Summary: Residual Connections**

| Feature                    | Benefit                               |
| -------------------------- | ------------------------------------- |
| Skip (shortcut) connection | Enables very deep models              |
| Adds input to output       | Learns residual (difference)          |
| Better gradient flow       | Solves vanishing gradients            |
| Identity function fallback | Avoids degradation when adding layers |

---

# ðŸ§  **Final Takeaway**

> **Residual connections** are the reason we can train **very deep networks** (e.g., 50â€“150 layers)
> without suffering from vanishing gradients or poor accuracy.

They are now **standard** in modern CNNs â€” and inspired many other deep learning architectures in vision and beyond.

# ðŸ§  **LeNet-5: The First Successful CNN**

## ðŸ“Œ **Overview**
- **Proposed by**: Yann LeCun et al.  
- **Year**: 1998  
- **Purpose**: Handwritten digit recognition (especially on the MNIST dataset)  
- **Significance**: Laid the foundation for modern convolutional neural networks.

---

# ðŸ§± **LeNet-5 Architecture Structure**

LeNet-5 consists of **7 layers** (excluding input), including convolutional, pooling, and fully connected layers. Here's the layer-by-layer breakdown:

| **Layer** | **Type**               | **Details**                            |
|-----------|------------------------|----------------------------------------|
| Input     | Image                  | `32Ã—32` grayscale image (MNIST images padded from 28Ã—28) |
| C1        | Convolutional Layer    | `6` filters of size `5Ã—5x1`, stride 1, no padding â†’ Output: `28Ã—28Ã—6` |
| S2        | Subsampling (Pooling)  | Average pooling with `2Ã—2` kernel, stride 2 â†’ Output: `14Ã—14Ã—6` |
| C3        | Convolutional Layer    | `16` filters of size `5Ã—5x6`, stride 1, no padding â†’ Output: `10Ã—10Ã—16` |
| S4        | Subsampling (Pooling)  | Average pooling with `2Ã—2` kernel, stride 2 â†’ Output: `5Ã—5Ã—16` |
| C5        | Convolutional Layer    | 120 filters of size `5Ã—5x16` (fully connected to all previous maps) â†’ Output: `1Ã—1Ã—120` |
| F6        | Fully Connected Layer  | 84 neurons (inspired by biological neural networks) |
| Output    | Fully Connected Layer  | 10 neurons (for digits 0â€“9), usually followed by softmax |

---

![image](https://github.com/user-attachments/assets/ba01ec62-dbce-48b1-b55b-5fd2af488f6a)


# ðŸ§  **Key Features and Innovations**

- **Convolution + Subsampling**: Pioneered the idea of using convolutional layers followed by pooling to extract features hierarchically.
- **Weight Sharing**: Reduces the number of parameters compared to fully connected networks.
- **Local Receptive Fields**: Mimics how the human visual system works â€” each neuron only sees a small portion of the input.
- **End-to-End Learning**: From raw image input to final classification, all weights are learned through backpropagation.

---

# ðŸ§ª **Training Details**

- **Loss Function**: Typically Mean Squared Error (originally); today, cross-entropy is preferred.
- **Activation Functions**: Originally tanh or sigmoid; modern implementations use ReLU.
- **Dataset**: MNIST â€” grayscale images of handwritten digits.

---

# ðŸ“Š **Impact of LeNet-5**

- One of the first networks to **successfully apply CNNs to real-world data**.
- Inspired major architectures like **AlexNet**, **VGG**, and **ResNet**.
- Still used today as a **teaching model** or **baseline** for small datasets.

---

# âœ… **Summary**

| **Aspect**        | **LeNet-5 Details**               |
|-------------------|-----------------------------------|
| Input Size        | 32Ã—32 grayscale                   |
| Total Parameters  | ~60,000                           |
| Output Classes    | 10 (digits 0â€“9)                   |
| Use Case          | Digit recognition (e.g., MNIST)   |
| Legacy            | Foundation of modern CNNs         |
